{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec3f8a89",
   "metadata": {},
   "source": [
    "# Decision Tree Tutorial\n",
    "\n",
    "## Overview\n",
    "\n",
    "Decision Tree is an algorithm that belongs to the supervised learning algorithms. It used for both classification and regression problems. The idea behind ecision tree is to create training model that can predict class(single or multi) or value by learning simple decision rules from training data. It works by recursively partitioning the data into subsets based on the values of the input features. Then a decision rule is applied to assign a label to the data at each step. The final result is a tree model that makes predictions on new data. \n",
    "\n",
    "Just like flowchart below, decision tree contains different types of nodes and branches. Every decision node represent the test on feature and based on the test result it will either form another branch or the leaf node. Every branch represents the decision rule and leaf node represent the final outcome.\n",
    "\n",
    "![Decision_Tree_Flowchart](Decision_Tree.png)\n",
    "\n",
    "**Types of decision tree:**\n",
    "* ***Classification decision trees*** - In this kind if decision trees, the decision variable is categorical.\n",
    "* ***Regression decision trees*** - In this kind of decision trees, the decision variable is continous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa35c9c",
   "metadata": {},
   "source": [
    "## Critiria To Split The Data\n",
    "\n",
    "The objective of decision tree is to split the data in such a way that at the ned we have different groups of data which has more similarity and less randomness/impurity. In order to achieve this, every split in decision tree must reduce the randomness. Decision tree uses ***entroty*** ir ***gini*** selection critiria to split the data.\n",
    "\n",
    "Note: for this tutorial, we are going to use sklearn library to test ***classification*** and ***regression***. 'entropy' or 'gini' critiria for classifier whereas ***mse***, ***friedman_mse***, and ***mae*** are selection critiria for regressor.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "In order to find the best feature which will reduce the randomness after a split, we can compare the randomness before and after the split for every feature. In the end we choose the feature which will provide the highest reduction in randomness. Formally randomness in data is known as 'Entropy' and difference between the 'Entropy' before and after split is known as 'Information Gain'.\n",
    "\n",
    "![entropy formula](entropy_formula.png)\n",
    "\n",
    "So, in the case of ***Entropy***, decision tree will split the data using the features that provides the highest information gain.\n",
    "\n",
    "\n",
    "### Gini\n",
    "\n",
    "In case of gini impurity, we pick a random data point in our dataset. Then randomly classify it according to the class distribution in the dataset. So it becomes very important to know the accuracy of this random classification. Gini impurity gives us the probability of incorrect classification. We’ll determine the quality of the split by weighting the impurity of each branch by how many elements it has. Resulting value is called as 'Gini Gain' or 'Gini Index'. This is what’s used to pick the best split in a decision tree. Higher the Gini Gain, better the split\n",
    "\n",
    "\n",
    "![gini formula](gini_formula.png)\n",
    "\n",
    "Sp, in the case of ***Gini***, decision tree will split the data using the features that provides the highest gini gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6815210b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
